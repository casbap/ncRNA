---
title: "DataPrep"
author: "Cas Baptist"
date: "3/7/2022"
output: html_document
---

### Introduction

set org = the organism you wish to test

The heirarchical clustering step needs a larger stack. Run this before executing:

`
ulimit -s 65533
`

### Prerequisite libraries

```{r Libraries, echo=TRUE}
suppressPackageStartupMessages({
    library(R.utils)
    library(dplyr)
    library(tidyr)
    library(data.table)
    library(RColorBrewer)
    library(gplots)
    library(edgeR)
    library(reshape2)
    library(Polychrome)
    library(dendextend)
    library(ggplot2)
    library(DESeq2)
    library(dynamicTreeCut)
    library(biomaRt)
    library(qdapTools)
    library(hrbrthemes)
    library(tibble)

})
source("functions.R")
```

### 1. Download and unzip the Human data 

```{r, Download Data, echo=TRUE}
# if statement checks if object exists
if ( !dir.exists("Data")) { dir.create("Data") }
if (!file.exists("Data/hsapiens_rand_se.tsv.bz2")){
    download.file("http://ziemann-lab.net/public/cas/rand/hsapiens_rand_se.tsv.bz2", 
              destfile = "Data/hsapiens_rand_se.tsv.bz2")}
if (!file.exists("Data/hsapiens_rand_se.tsv")){
    bunzip2("Data/hsapiens_rand_se.tsv.bz2", destname="Data/hsapiens_rand_se.tsv")}
orgData <- fread("Data/hsapiens_rand_se.tsv", sep = '\t', header = FALSE)
head(orgData)
```

### 2. Download, read, and unzip the QC data

```{r, QC Data, echo=TRUE}
if (!file.exists("hsapiens_rand_qc.tsv.bz2")){
    download.file("http://ziemann-lab.net/public/cas/rand/hsapiens_rand_qc.tsv.bz2", 
              destfile = "hsapiens_rand_qc.tsv.bz2")}
qcData <- fread("hsapiens_rand_qc.tsv.bz2", sep = '\t', 
  col.names = c("Database", "Description", "Details"))
head(qcData)
```

### 3. Filter and quantify databases according to qcSummary = PASS, WARN, or FAIL.  

```{r, QC Data Summary, echo=TRUE}
# Count the total number of qcSummary entries for checking
qcSummary <- qcData[qcData$Description == 'qcSummary',]
if ( nrow(qcSummary) > 0 ) {  stop("Quality check data not loaded accurately") }
totalSummary <- nrow(qcSummary)

# Count the total number of databases marked as PASS
qcPass <- filter(qcData, grepl("PASS", Details))
totalPass <- nrow(qcPass)
# Count the total number of databases marked as WARN
qcWarn <- filter(qcData, grepl("WARN", Details))
totalWarn <- nrow(qcWarn)
# Count the total number of databases marked as FAIL
qcFail <- filter(qcData, grepl("FAIL", Details))
totalFail <- nrow(qcFail)
# Summary of counts. Addition of each PASS, WARN, and FAIL counts equals to total_summary.
qualitySummary <- data.frame(totalPass, totalWarn, totalFail, totalSummary)
qualitySummary
```

### 4. Select all the databases from the quality control metrics data with qcSummary == 'PASS' 

```{r, Filter org bulk data, echo=TRUE}
# Filter all the sample names with qcSummary == 'PASS' from step 3
databasePass <- filter(qcData, grepl("PASS", Details)) 
# Convert the rows into string and store on a list to use for filtering org data
databasePassList <- as.character(databasePass$Database)
str(databasePassList)
saveRDS(databasePassList, "databasePassList")
# Filter org data using generated list
orgPass <- orgData[orgData$V1 %in% databasePassList,] 
# Convert format from long to wide
orgPassWide <- orgPass %>% pivot_wider(names_from = "V1", values_from = "V3")
# Convert tibble to data frame and assign column 1 as rowname
orgPassWide <- as.data.frame(orgPassWide)
rownames(orgPassWide) <- orgPassWide[,1]
orgPassWide <- orgPassWide[,-1]
orgPassWide[1:10,1:6]
if ( nrow(orgPassWide) < 10 ) {  stop("Data too small") }
saveRDS(orgPassWide, "orgPassWide.rds")

```

### 5. Aggregate multiple runs, if any, (SRR) to its corresponding experiment (SRX) with PASS only databases

Ensure all chunks have been previously loaded, as srx_agg function requires the function.R source, multiple libraries, and databasePassList. 

```{r, SRR to SRX Aggregation, echo=TRUE}
# Download and read Metadata summary
if (!file.exists("hsapiens_metadata.tsv.cut")){
    download.file("http://ziemann-lab.net/public/cas/rand/hsapiens_metadata.tsv.cut", 
              destfile = "hsapiens_metadata.tsv.cut")}
    
orgMetadata <- read.csv("hsapiens_metadata.tsv.cut", sep = '\t')
# Filter metadata to include only "passed" samples
orgMetadataPass <- orgMetadata[which(orgMetadata$SRR_accession %in% databasePassList),]
# Assign column 1 as rownames 
rownames(orgMetadataPass) <- orgMetadataPass[,1]
orgMetadataPass <- orgMetadataPass[,-1]
orgMetadataPass
saveRDS(orgMetadataPass, "orgMetadataPass.rds")

# Put the filtered org GeneCount data and the filtered metadata dataframes into a list
orgCountMetadata <- list(GeneCounts = orgPassWide, MetadataSummary = orgMetadataPass)

# Apply both dataframes to the function
head(orgCountMetadata)

#checks to see if databasePassList correctly saved to prevent memory usage issue during aggregation stage
if(!exists("databasePassList") ){stop ("databasePassList not loaded, check previous chunk")}

# Aggregation stage with srx_agg function
agg <- srx_agg(orgCountMetadata)

# Remove genes with zero or more total counts 
length(which(rowSums(agg) == 0))
no_gene_counts <- names(which(rowSums(agg) == 0))
aggFilterRows <- agg[which(rowSums(agg) > 0), ]
agg <- agg[!(rownames(agg) %in% no_gene_counts),]

# Check for samples with zero total counts
colSums(agg) == 0

# Filtering samples with more than 1 million reads
aggFilterCols <- agg[, which(colSums(agg) >= 1e6)]

# Omit rows and columns with more than zero counts
dim(agg)
if ( nrow(agg) == 0 ) { stop("Aggregation failure - Zero count detected") }

# Save agg object
saveRDS(agg, "agg.rds")

# Histogram showing samples (columns) counts
hist(colSums(agg), breaks = 20)

```

### 6. Normalisation of Data (with PASS only databases)

```{r, normalisation, echo=TRUE}
# Filtering low counts
# Check if there are genes with no counts (rowSums==0)
length(which(rowSums(agg) == 0)) # no rowSums = 0
if (length(which(rowSums(agg) == 0)) > 0 ) {  stop("PASS data not normalised") }

#NORMALISATION FOR COMPOSITION BIAS (TMM normalisation)
dgeObj <- DGEList(agg)
# Apply TMM normalisation to DGEList object
dgeObj <- calcNormFactors(dgeObj, method = "TMM") 
normAgg <- cpm(dgeObj)

# Scale after normalisation
normAggScale <- scale(normAgg)

# Check distributions of samples
hist(normAggScale, breaks = 10)
saveRDS(normAggScale,"normAggScale.rds")

# Get, and convert to log2 counts per million
normAggLog <- cpm(dgeObj,log=TRUE)
hist(normAggLog, breaks = 10)
saveRDS(normAggLog,"normAggLog.rds")


```

### 6b. Correlation matrix creation

``` {r, Correlate}

# Correlation Matrix
correlate <- cor(t(normAggScale), method = "spearman")
correlate[upper.tri(correlate,diag=TRUE)] <- NA

#extract values and combine for large matrices
if(nrow(correlate)>30000) {
  correlate1 <- correlate[1:30000,]
  correlate2 <- correlate[30001:nrow(correlate),]
  correlate_melt1 <- reshape2::melt(correlate1, na.rm = TRUE)
  correlate_melt2 <- reshape2::melt(correlate2, na.rm = TRUE)
  correlate_melt <- rbind(correlate_melt1, correlate_melt2)
  remove(correlate_melt1)
  remove(correlate_melt2)
} else {
  correlate_melt <- reshape2::melt(correlate, na.rm = TRUE)
}

# Check for 0 values
length(which(correlate_melt$value ==0 )) # no zero values
if (length(which(correlate_melt$value ==0 )) > 0 ) {  stop("large matrix combination error") }
head(correlate_melt)
saveRDS(correlate_melt, "correlate_melt")

#Spearman Correlation of RNASeq Counts
png("Data/hist_correlation1.png", width = 5*300, height = 5*300, res = 300, pointsize = 8)
hist(correlate_melt, breaks = 20, xlab = "Gene Correlation Values", 
  main = "Spearman Correlation of RNASeq Counts")
dev.off()

```

### 7a. Hierarchical Clustering: linking method

There are different ways of deciding the distance of elements for them to be grouped into a cluster. The correlation between the distance matrix and the cophenetic distance is calculated to to ensure good
linkage method fit. 

```{r, hierarchical_clustering, echo=TRUE}
set.seed(42)
distClust <-as.dist(1-cor(t(normAggScale), method="spearman"))
hClust <- hclust(distClust , method="complete")

#convert hClust to dataframe
hClust <- as.data.frame(hClust, row.names = "Gene IDs", optional = FALSE,
              col.names = "Runs (Metadata)", fix.empty.names = TRUE,
              stringsAsFactors = default.stringsAsFactors())
saveRDS(hClust, "hClust")
head(hClust)

# Check the correlation between the distance matrix and the cophenetic distance [test case]
cophenetic.d = cophenetic(hClust)
correlate_distClustVcoph <- cor(distClust, cophenetic.d)

#form dendrogram
png("Data/dendro_Average.png", width = 5*300, height = 5*300, res = 300, pointsize = 8)
  dend <- as.dendrogram(hClust)
  labels_colors(dend) <- "white"
  plot(dend, main = paste0("Complete Method: Corr = ", signif(correlate_distClustVcoph)))
print(dend)
dev.off()
```

### 7b. Heirarchical Clustering: deciding the number of clusters

A specific number of clusters can be set using the cutree function; let k = chosen cluster number.

```{r, heirarchical1, echo=TRUE}

# 
# Using the argument k in cutree is a faster process vs figuring out the cut height
cutClust <- cutree(hClust, k=200)
cutClustlength <- length(unique(cutClust))
saveRDS(cutClust, "cutClust")

# Cluster size check statement
cutClustlength
if (length(cutClustlength ==0 )) {  stop("cutree function could not find appropriate cluster lengths") }
  
```

### 8. Clustering genes using different cluster sizes 

This functions will will yield a list of genes grouped in cluster sized of 50 to 2000 to be 
used in downstream processes.

leave eval=FALSE otherwise chunk requires manual cutHeight addition

```{r, clustering1, eval=FALSE }

# Different Cluster sizes using the dynamic method, cutreeHybrid from the dynamicTreeCut library
cutClustvalues_dynamic <- cl_cut_dynamic(hr=hClust, cl=distClust, min=4, max=302, interval=2)
saveRDS(cutClustvalues_dynamic, "cutClustvalues_dynamic.rds")

```

### 9. Heatmaps for the clusters

```{r, heatmaps1}

# Visualisation parameters and colouring
if (!exists("P100")){
  P100 <- createPalette(100,  c("#yellow", "#orange", "#red"))}
clusterCols <- P100[1:cutClustvalues_dynamic]
colfunc <- colorRampPalette(c("darkgoldenrod2","darkorange2","darkred"))

# Heatmaps 
png("Data/normalised_clusters.png", width = 5*300, height = 5*300, res = 300, pointsize = 8)
cutClusterSideBar <- clusterCols[cutClustvalues_dynamic[["Cluster Values"]]]
heatmap(correlate_melt, main="Gene Correlation (TMM, Spearman)",  Rowv=as.dendrogram(hClust),
         dendrogram="both", scale="col", col = colfunc(25), trace="none",
         RowSideColors= cutClustusterSideBar, margins = c(5,5))

dev.off()

```

### Data Removal

Removes data functions to improve memory storage for imputation stages

```{r, data_removal, echo=FALSE}
remove(cophenetic.d)
remove(correlate_distClustVcoph)
remove(dend)
remove(orgPass)
remove(databasePass)
remove(qcSummary)
remove(totalSummary)
remove(qcWarn)
remove(totalWarn)
remove(qcFail)
remove(totalFail)
remove(orgMetadata)
remove(databasePassList)
remove(dgeObj)
remove(distClust)
```

### Mem usage

```{r,memusage}
sort( sapply( ls() , function(x) { object.size( get( x ) ) }  )  )
```

### Session Information 

```{r Session Info, echo=FALSE}
sessionInfo()
```
